# ğŸš§ Data Warehouse - AnÃ¡lise de Acidentes RodoviÃ¡rios Federais

Projeto desenvolvido como atividade avaliativa para a disciplina de **TÃ³picos AvanÃ§ados de Banco de Dados (TABD)**, no curso de **Bacharelado em Sistemas de InformaÃ§Ã£o** do **Instituto Federal de Alagoas (IFAL) - Campus MaceiÃ³**.

## ğŸ“Œ Objetivos

Construir um Data Warehouse (DW) voltado Ã  anÃ¡lise de dados dos acidentes ocorridos nas rodovias federais brasileiras, com foco na identificaÃ§Ã£o de:

- Quantidade de feridos leves, feridos graves e Ã³bitos.
- Rodovias e trechos (KM) com maior incidÃªncia de acidentes.
- Principais causas dos acidentes.

## ğŸ› ï¸ Ferramentas Utilizadas

- ğŸ§© **Oracle SQL Developer Data Modeler** â€“ para a modelagem relacional e dimensional do DW.
- ğŸ—„ï¸ **Oracle SQL Developer** â€“ para criaÃ§Ã£o e execuÃ§Ã£o dos scripts no banco de dados.
- ğŸ“¦ **Pentaho Data Integration** â€“ para processos de **ETL (Extract, Transform, Load)**.
- ğŸ“Š **Microsoft Power BI** â€“ para construÃ§Ã£o de **dashboards interativos**.
- ğŸ“ **Dados da PRF (gov.br)** â€“ utilizados como base primÃ¡ria em formato `.csv`.

## ğŸ§  Modelagem Dimensional

A modelagem adotada foi o Star Schema (Esquema Estrela), com uma tabela fato central contendo os dados quantitativos e seis tabelas dimensÃ£o com informaÃ§Ãµes descritivas.

### ğŸ“Œ Tabela Fato:
- Quantidade de acidentes.
- Feridos (leves e graves).
- Ã“bitos.

### ğŸ“ Tabelas DimensÃ£o:
- `dim_data`: data do acidente (ano, mÃªs, dia, etc.).
- `dim_periodo`: perÃ­odo do dia (dia, noite, amanhecer, etc.).
- `dim_local`: estado, municÃ­pio, BR e quilÃ´metro, longitude, latitude.
- `dim_causa`: causa do acidente.
- `dim_tipo`: tipo de acidente (colisÃ£o frontal, atropelamento, etc.).
- `dim_clima`: condiÃ§Ã£o climÃ¡tica no momento do acidente (chuva, sol, etc.).

![image](https://github.com/user-attachments/assets/ed736439-6a30-4720-a4ab-c89e9b5417ea)

## ğŸ”„ Processo de ETL

O processo de ETL foi automatizado com o Pentaho. Os dados foram extraÃ­dos de arquivos `.csv` da PRF e dos estados brasileiros, limpos, transformados e carregados no Data Warehouse com:

- Tratamento de dados.
- CriaÃ§Ã£o automÃ¡tica de chaves substitutas (surrogate keys).
- Lookup nas dimensÃµes para gerar a tabela fato.
- Job unificado para executar todas as transformaÃ§Ãµes com um clique.

## ğŸ“Š Dashboards

Dois dashboards interativos foram criados no Power BI para anÃ¡lise dos dados de acidentes a partir das informaÃ§Ãµes carregados no banco de dados:

### ğŸ”¹ Dashboard 1 â€“ VisÃ£o Geral

- IdentificaÃ§Ã£o do local do acidente no mapa interativo atravÃ©s de dados de latitude/longitude (incluso no dashboard 2).
- Tabela com o total de mortos, feridos leves/graves.
- Ranking das cidades com mais acidentes.
- GrÃ¡ficos de linha por tempo (ano, mÃªs, etc.).
- Filtros por data, estado, BR, KM, tipo de acidente, causa do acidente, perÃ­odo do dia e clima (incluso no dashboard 2).


![image](https://github.com/user-attachments/assets/9518ab59-281d-4933-bd80-9df21c52a6f4)

### ğŸ”¹ Dashboard 2 â€“ Rodovias e Causas

- Tabela com os quilÃ´metros de cada BR com mais acidentes.
- Ranking de causas mais frequentes.


![image](https://github.com/user-attachments/assets/0fa8d4b8-9e0c-47b3-84f1-93907e5cb278)

> âš ï¸ Os dados utilizados foram referentes ao ano de **2023**, com mais de **67 mil registros**.

## ğŸ“¦ Como Executar o Projeto na Sua MÃ¡quina

### âœ… Requisitos

Certifique-se de ter os softwares instalados:

| Software | VersÃ£o mÃ­nima | ObservaÃ§Ãµes |
|----------|---------------|-------------|
| [Oracle Database](https://www.oracle.com/br/database/) | 21c | Usado para o Data Warehouse |
| [SQL Developer](https://www.oracle.com/tools/downloads/sqldev-downloads.html) | Qualquer | Para executar os scripts SQL |
| [Pentaho Data Integration (Kettle)](https://sourceforge.net/projects/pentaho/) | 9.x | Para rodar o ETL |
| [Power BI Desktop](https://powerbi.microsoft.com/pt-br/desktop/) | Qualquer | Para abrir os dashboards localmente |
| [Java (JDK)](https://www.oracle.com/java/technologies/downloads/) | 8 ou 11 |

### ğŸ“¥ Passo 1: Clonar o RepositÃ³rio

```bash
git clone https://github.com/deivisondelmiro/tabd
cd tabd
```

### ğŸ§± Passo 2: Confirme a seguinte estrutura das pastas:
```bash
â”œâ”€â”€ carga\ - TransformaÃ§Ãµes (.ktr) e job (.kjb) do Pentaho
â”œâ”€â”€ csv\ - Dados de entrada em CSV
â”œâ”€â”€ script\ - Script de criaÃ§Ã£o do DW
```

### ğŸ› ï¸ Passo 3: Criar o Banco de Dados no Oracle

```bash
Abra o SQL Developer.

Crie um novo usuÃ¡rio chamado sch_acd com a senha sch_acd.

Execute o script SQL localizado em:

C:\tabd_acidentes\script\script.sql
```

### ğŸ”— Passo 4: ConexÃ£o no Pentaho

```bash
Abra o Pentaho Data Integration e procure por conexÃµes, confirmando as informaÃ§Ãµes abaixo:

Banco: Oracle
Nome do servidor: localhost
Porta: 1521
UsuÃ¡rio: sch_acd
Senha: sch_acd
Nome do banco: /XEPDB1 (ou o nome que vocÃª usa)
```

### ğŸ”„ Passo 5: Executar o ETL

```bash
No Pentaho:

Abra o Job Principal (job.kjb) em C:\tabd_acidentes\carga\job.kjb

Verifique os caminhos dos arquivos .csv em cada transformaÃ§Ã£o (.ktr).

Clique no botÃ£o Start (Play).

O processo irÃ¡: carregar todas as dimensÃµes, Carregar a tabela fato, opular todo o DW.
````

### ğŸ“Š Visualizar os Dashboards

```bash
Abra o Power BI Desktop.

Importe o arquivo .pbix localizado na pasta /BI do projeto.

Os dashboards estÃ£o prontos para uso.
```
